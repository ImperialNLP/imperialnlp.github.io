@article{alazraki2023not,
  html = {https://arxiv.org/abs/2310.06641},
  abbr = { NLP },
  bibtex_show = {true},
  title={How (not) to ensemble LVLMs for VQA},
  author={Alazraki, Lisa and Castrejon, Lluis and Dehghani, Mostafa and Huot, Fantine and Uijlings, Jasper and Mensink, Thomas},
  journal={Proceedings of Machine Learning Research (PMLR) on ICBINB @ NeurIPS 2023},
  volume={239},
  pages={1--20},
  year={2023}
}

@article{alazraki2024meta,
  html = {https://arxiv.org/abs/2411.04535},
  abbr = { NLP },
  bibtex_show = {true},
  title={Meta-Reasoning Improves Tool Use in Large Language Models},
  author={Alazraki, Lisa and Rei, Marek},
  journal={NAACL 2025 Findings},
  year={2024}
}

@article{alazraki2025llms,
  html = {https://arxiv.org/abs/2502.08550},
  abbr = { NLP },
  bibtex_show = {true},
  title={LLMs can implicitly learn from mistakes in-context},
  author={Alazraki, Lisa and Mozes, Maximilian and Campos, Jon Ander and Tan, Yi Chern and Rei, Marek and Bartolo, Max},
  journal={arXiv preprint arXiv:2502.08550},
  year={2025}
}

@inproceedings{xu2025can,
  html = {https://arxiv.org/abs/2503.04377},
  abbr = { NLP },
  bibtex_show = {true},
  title={How can representation dimension dominate structurally pruned LLMs?},
  author={Xu, Mingxue and Alazraki, Lisa and Mandic, Danilo P},
  booktitle={ICLR 2025 Workshop on Sparsity in LLMs (SLLM)},
  year={2025}
}

@inproceedings{agrawal2025enhancing,
  html = {https://arxiv.org/abs/2504.02733},
  abbr = { NLP },
  bibtex_show = {true},
  title={Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study},
  author={Agrawal, Aryan and Alazraki, Lisa and Honarvar, Shahin and Rei, Marek},
  booktitle={ICLR 2025 Workshop on Building Trust in Language Models and Applications},
  year={2025}
}