@inproceedings{stacey2020avoiding,
  title={Avoiding the hypothesis-only bias in natural language inference via ensemble adversarial training},
  author={Stacey, Joe and Minervini, Pasquale and Dubossarsky, Haim and Riedel, Sebastian and Rockt{\"a}schel, Tim},
  booktitle={EMNLP 2020},
  year={2020}
}

@inproceedings{stacey2022supervising,
  title={Supervising Model Attention with Human Explanations for Robust Natural Language Inference},
  author={Stacey, Joe and Belinkov, Yonatan and Rei, Marek},
  booktitle={AAAI 2022},
  year={2022}
}

@inproceedings{stacey2022logical,
  title={Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models},
  author={Stacey, Joe and Minervini, Pasquale and Dubossarsky, Haim and Rei, Marek},
  booktitle={EMNLP 2022},
  year={2022}
}

@inproceedings{stacey2023atomic,
  title={Atomic Inference for NLI with Generated Facts as Atoms},
  author={Stacey, Joe and Minervini, Pasquale and Dubossarsky, Haim and Camburu, Oana-Maria and Rei, Marek},
  booktitle={EMNLP 2024},
  year={2023}
}

@article{stacey2023distilling,
  title={Distilling Robustness into Natural Language Inference Models with Domain-Targeted Augmentation},
  author={Stacey, Joe and Rei, Marek},
  journal={ACL Findings 2024},
  year={2023}
}

@inproceedings{ravichander2023and,
  title={When and why does bias mitigation work?},
  author={Ravichander, Abhilasha and Stacey, Joe and Rei, Marek},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={9233--9247},
  year={2023}
}